{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse_Image_Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various Imports that were required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL imports needed\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import imghdr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from skimage.feature import hog\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar-10-batches-py/data_batch_1\", \"rb\") as file:\n",
    "    data = pickle.load(file, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"data\"].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(data['data'])\n",
    "print(images.shape)\n",
    "images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "first_image = images[0]\n",
    "plt.imshow(first_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image proccessing/Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(data['data']).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images):\n",
    "    return images\n",
    "#Will come beack to preproccessing images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(image0):\n",
    "    image = image0.astype(float)\n",
    "    gradient_magnitude = []\n",
    "    gradient_orientation = []\n",
    "    grad_x = []\n",
    "    grad_y = []    \n",
    "    for i in range(3):\n",
    "        #image = np.mean(image0, axis = 2)\n",
    "        # #print(image.shape, image[0].shape)\n",
    "        sobel_x = np.diff(image[:,:,i], axis = 0)\n",
    "        sobel_x = np.vstack([sobel_x,np.zeros((1,image.shape[0]))])\n",
    "        #sobel_x = np.concatanate([sobel_x,])\n",
    "        sobel_y = np.diff(image[:,:,i], axis = 1)\n",
    "        sobel_y = np.hstack([sobel_y,np.zeros((image.shape[1],1))])\n",
    "        print(sobel_x.shape,sobel_y.shape)\n",
    "        gradient_magnitude.append(np.sqrt(sobel_x**2 + sobel_y**2))\n",
    "        gradient_orientation.append(np.arctan2(sobel_y, sobel_x))\n",
    "        grad_x.append(sobel_x)\n",
    "        grad_y.append(sobel_y)\n",
    "    gradient_magnitude = np.stack(gradient_magnitude, axis =2)\n",
    "    gradient_orientation = np.stack(gradient_orientation, axis =2)\n",
    "    grad_x = np.stack(grad_x, axis =2)\n",
    "    grad_y = np.stack(grad_y, axis =2)\n",
    "    return gradient_magnitude, gradient_orientation, grad_x.astype(float), grad_y.astype(float)\n",
    "\n",
    "gradient_magnitude, gradient_orientation, sobel_x, sobel_y = calculate_gradients(images[50])\n",
    "gradient_magnitude.shape, gradient_orientation.shape, sobel_x.shape, sobel_y.shape\n",
    "gradient_magnitude.dtype, gradient_orientation.dtype, sobel_x.dtype, sobel_y.dtype\n",
    "images[50].dtype\n",
    "sobel_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 50\n",
    "preprocessed_image = preprocess(images[image_index])\n",
    "gradient_magnitude, gradient_orientation, sobel_x, sobel_y = calculate_gradients(preprocessed_image)\n",
    "\n",
    "# Visualize the gradients and the original image\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(np.abs(sobel_x),cmap = 'jet')\n",
    "plt.title(\"Absolute Value of X-Gradient\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(np.abs(sobel_y),cmap = 'jet')\n",
    "plt.title(\"Absolute Value of Y-Gradient\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(gradient_magnitude, cmap = 'jet')\n",
    "plt.title(\"Magnitude of Gradient\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(preprocessed_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hog_features(gradient_magnitude, gradient_orientation):\n",
    "    hog_features = hog(gradient_magnitude, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_feature_vectors_r = []\n",
    "hog_feature_vectors_g = []\n",
    "hog_feature_vectors_b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    print(image.shape)\n",
    "    #gradient_magnitude_r, gradient_orientation_r, _,_ = calculate_gradients(image[:, :, 0])\n",
    "    #gradient_magnitude_g, gradient_orientation_g,  _,_  = calculate_gradients(image[:, :, 1])\n",
    "    #gradient_magnitude_b, gradient_orientation_b,  _,_  = calculate_gradients(image[:, :, 2])\n",
    "\n",
    "    gradient_magnitude, gradient_orientation, _,_ = calculate_gradients(image)\n",
    "\n",
    "    gradient_magnitude_r = gradient_magnitude[:, :, 0]\n",
    "    gradient_orientation_r = gradient_orientation[:, :, 0]\n",
    "\n",
    "    gradient_magnitude_g = gradient_magnitude[:, :, 1]\n",
    "    gradient_orientation_g = gradient_orientation[:, :, 1]\n",
    "\n",
    "    gradient_magnitude_b = gradient_magnitude[:, :, 2]\n",
    "    gradient_orientation_b = gradient_orientation[:, :, 2]\n",
    "\n",
    "    hog_features_r = calculate_hog_features(gradient_magnitude_r, gradient_orientation_r)\n",
    "    hog_features_g = calculate_hog_features(gradient_magnitude_g, gradient_orientation_g)\n",
    "    hog_features_b = calculate_hog_features(gradient_magnitude_b, gradient_orientation_b)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    hog_features_r = scaler.fit_transform(hog_features_r.reshape(-1, 1)).reshape(-1)\n",
    "    hog_features_g = scaler.fit_transform(hog_features_g.reshape(-1, 1)).reshape(-1)\n",
    "    hog_features_b = scaler.fit_transform(hog_features_b.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    hog_feature_vectors_r.append(hog_features_r)\n",
    "    hog_feature_vectors_g.append(hog_features_g)\n",
    "    hog_feature_vectors_b.append(hog_features_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_feature_vectors_r = np.array(hog_feature_vectors_r)\n",
    "hog_feature_vectors_g = np.array(hog_feature_vectors_g)\n",
    "hog_feature_vectors_b = np.array(hog_feature_vectors_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_hog_features = (np.array(hog_feature_vectors_r) + np.array(hog_feature_vectors_g) + np.array(hog_feature_vectors_b)) / 3.0\n",
    "scalar_average = StandardScaler()\n",
    "normalised_hog_features = scaler.fit_transform(average_hog_features)\n",
    "\n",
    "print(\"Shape of the averages normalised: \", normalised_hog_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['labels']\n",
    "train_size = 8000\n",
    "train_data = average_hog_features[:train_size:]\n",
    "train_labels = labels[:train_size]\n",
    "test_data = average_hog_features[train_size:, :]\n",
    "test_labels = labels[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "\n",
    "gamma = 'scale'\n",
    "c_hyperparameter = 10\n",
    "kernel_option = 'rbf'\n",
    "svm_classifier = SVC(kernel= kernel_option, C=c_hyperparameter, gamma=gamma, probability=True)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('svm', svm_classifier)\n",
    "])\n",
    "pipeline.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test_data)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "confusion = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "#F1 scores \n",
    "f1_scores = f1_score(test_labels, predictions, average=None)\n",
    "overall_f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# print F1 scores \n",
    "print(\"F1 Scores for Each Class:\")\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    print(f\"Class: {class_name}, F1 Score: {f1_scores[class_idx]:.2f}\")\n",
    "print(\"Overall F1 Score:\", overall_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities for each class label\n",
    "probabilities = pipeline.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN pipeline\n",
    "k_neighbors = 5  # Set the number of neighbors for KNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('knn', knn_classifier)\n",
    "])\n",
    "knn_pipeline.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test_data)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "confusion = confusion_matrix(test_labels, combined_predicted_classes)\n",
    "#F1 scores\n",
    "f1_scores = f1_score(test_labels, combined_predicted_classes, average=None)\n",
    "overall_f1 = f1_score(test_labels, combined_predicted_classes, average='weighted')\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# show F1 scores \n",
    "print(\"F1 Scores for Each Class:\")\n",
    "for class_index, class_name in enumerate(class_names):\n",
    "    print(f\"Class: {class_name}, F1 Score: {f1_scores[class_index]:.2f}\")\n",
    "print(\"Overall F1 Score:\", overall_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for test data using the KNN pipeline\n",
    "probabilities = knn_pipeline.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities from both SVM and KNN models\n",
    "svm_probabilities = pipeline.predict_proba(test_data)\n",
    "knn_probabilities = knn_pipeline.predict_proba(test_data)\n",
    "# defined weights as svm is a better method more weight was given\n",
    "svm_weight = 0.7\n",
    "knn_weight = 0.3\n",
    "combined_probabilities = svm_probabilities * svm_weight + knn_probabilities * knn_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes based on the combined probabilities\n",
    "combined_predicted_classes = top_classes[:, -1]\n",
    "accuracy = accuracy_score(test_labels, combined_predicted_classes)\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "confusion = confusion_matrix(test_labels, combined_predicted_classes)\n",
    "#  F1 scores\n",
    "f1_scores = f1_score(test_labels, combined_predicted_classes, average=None)\n",
    "overall_f1 = f1_score(test_labels, combined_predicted_classes, average='weighted')\n",
    "\n",
    "#Display all metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"F1 Scores for Each Class:\")\n",
    "for class_name, f1 in zip(class_names, f1_scores):\n",
    "    print(f\"Class: {class_name}, F1 Score: {f1:.2f}\")\n",
    "print(\"Overall F1 Score:\", overall_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a search (query image) and output similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query image index from the testing set\n",
    "query_index = np.random.randint(len(test_data))\n",
    "query_image = np.reshape(data['data'][query_index], (3, 32, 32)).transpose(1, 2, 0)\n",
    "\n",
    "#Prediction from the query image and what the query image \n",
    "query_prediction = combined_predicted_classes[query_index]\n",
    "query_class_name = class_names[query_prediction]\n",
    "plt.imshow(query_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(query_class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distnace between query image and other images and pulling the most simiar images\n",
    "distances = pairwise_distances(query_image.flatten().reshape(1, -1), images.reshape(images.shape[0], -1))\n",
    "closest_indices = np.argsort(distances.flatten())[1:6]\n",
    "\n",
    "# Display the closest images\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, idx in enumerate(closest_indices):\n",
    "    closest_image = images[idx]\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(closest_image)\n",
    "    plt.title(f\"Closest Image {i + 1}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
